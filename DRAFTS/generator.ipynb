{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sequence', 'extra_name', 'cpp_category', 'is_cpp', 'cpp_type',\n",
       "       'origin', 'id_uptake', 'peptide', 'uptake_type', 'raw_efficiency',\n",
       "       'raw_toxicity', 'raw_concentration', 'id_experiment',\n",
       "       'peptide_experiment', 'raw_time', 'method', 'cell_line', 'cargo',\n",
       "       'mechanism', 'raw_temperature', 'id_article', 'doi', 'pubmed_id',\n",
       "       'title', 'sequence_category', 'standard_sequence', 'nh3_tail',\n",
       "       'po3_pos', 'biotinylated', 'acylated_n_terminal', 'cyclic', 'amidated',\n",
       "       'stearyl_uptake', 'hexahistidine_tagged', 'modifications',\n",
       "       'smiles_sequence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/dreamtim/Coding/ITMO/itmo-cpp/output_data/all_peptides_with_smiles.csv', index_col=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_cpp\n",
       "True     1601\n",
       "False    1321\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_cpp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined DataFrame with essential columns\n",
    "df_working = df[['smiles_sequence', 'is_cpp']].copy()\n",
    "\n",
    "# Filter df: only not na and not empty smiles_sequence\n",
    "df_working = df_working.dropna(subset=['smiles_sequence'])\n",
    "\n",
    "# First split: train (80%) vs temp (20%)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_working,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_working['is_cpp']  # Maintain class balance\n",
    ")\n",
    "\n",
    "# Second split: validation (10%) and test (10%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['is_cpp']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_condition_token(row):\n",
    "    return f\"[CPP]{row['smiles_sequence']}\" if row['is_cpp'] else f\"[NON]{row['smiles_sequence']}\"\n",
    "\n",
    "for df_split in [train_df, val_df, test_df]:\n",
    "    df_split['conditional_smiles'] = df_split.apply(add_condition_token, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CPP ratio: 0.55\n",
      "Val CPP ratio: 0.55\n",
      "Test CPP ratio: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train CPP ratio: {train_df['is_cpp'].mean():.2f}\")\n",
    "print(f\"Val CPP ratio: {val_df['is_cpp'].mean():.2f}\")\n",
    "print(f\"Test CPP ratio: {test_df['is_cpp'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers\n",
    "\n",
    "# Train BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "trainer = trainers.BpeTrainer(\n",
    "    special_tokens=[\"[PAD]\", \"[CPP]\", \"[NON]\"],\n",
    "    min_frequency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens and post-processing\n",
    "tokenizer.add_special_tokens([\"[CPP]\", \"[NON]\", \"[PAD]\"])\n",
    "\n",
    "# Set padding and truncation\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "    pad_token=\"[PAD]\",\n",
    "    length=128  # Adjust based on your max SMILES length\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CPPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_sequences):\n",
    "        self.sequences = tokenized_sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return pad_sequence(\n",
    "        batch,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "# Tokenize all splits\n",
    "train_enc = [tokenizer.encode(s).ids for s in train_df['conditional_smiles']]\n",
    "val_enc = [tokenizer.encode(s).ids for s in val_df['conditional_smiles']]\n",
    "\n",
    "train_dataset = CPPDataset(train_enc)\n",
    "val_dataset = CPPDataset(val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CPPGenerator were not initialized from the model checkpoint at ncfrey/ChemGPT-19M and are newly initialized: ['transformer.cpp_embed.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPTNeoModel\n",
    "\n",
    "\n",
    "# 1. Load pre-trained chemical model\n",
    "base_model = GPTNeoModel.from_pretrained(\"ncfrey/ChemGPT-19M\")\n",
    "\n",
    "# 2. Modify for CPP generation\n",
    "class CPPGenerator(base_model.__class__):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Add CPP-specific conditioning\n",
    "        self.cpp_embed = torch.nn.Embedding(2, config.hidden_size)  # 0=non-CPP, 1=CPP\n",
    "\n",
    "# 3. Initialize with pre-trained weights\n",
    "model = CPPGenerator.from_pretrained(\"ncfrey/ChemGPT-19M\", config=base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoPreTrainedModel, GPTNeoModel\n",
    "\n",
    "class CPPGenerator(GPTNeoPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = GPTNeoModel(config)\n",
    "        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Add CPP-specific embedding (0=non-CPP, 1=CPP)\n",
    "        self.cpp_embed = torch.nn.Embedding(2, config.hidden_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, cpp_labels=None):\n",
    "        # Get hidden states from ChemGPT\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # If CPP labels are provided, add the CPP embedding\n",
    "        if cpp_labels is not None:\n",
    "            cpp_embeddings = self.cpp_embed(cpp_labels).unsqueeze(1)  # Shape: (batch, 1, hidden_size)\n",
    "            hidden_states = hidden_states + cpp_embeddings\n",
    "\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CPPGenerator were not initialized from the model checkpoint at ncfrey/ChemGPT-19M and are newly initialized: ['transformer.cpp_embed.weight', 'transformer.lm_head.weight', 'transformer.model.h.0.attn.attention.k_proj.weight', 'transformer.model.h.0.attn.attention.out_proj.bias', 'transformer.model.h.0.attn.attention.out_proj.weight', 'transformer.model.h.0.attn.attention.q_proj.weight', 'transformer.model.h.0.attn.attention.v_proj.weight', 'transformer.model.h.0.ln_1.bias', 'transformer.model.h.0.ln_1.weight', 'transformer.model.h.0.ln_2.bias', 'transformer.model.h.0.ln_2.weight', 'transformer.model.h.0.mlp.c_fc.bias', 'transformer.model.h.0.mlp.c_fc.weight', 'transformer.model.h.0.mlp.c_proj.bias', 'transformer.model.h.0.mlp.c_proj.weight', 'transformer.model.h.1.attn.attention.k_proj.weight', 'transformer.model.h.1.attn.attention.out_proj.bias', 'transformer.model.h.1.attn.attention.out_proj.weight', 'transformer.model.h.1.attn.attention.q_proj.weight', 'transformer.model.h.1.attn.attention.v_proj.weight', 'transformer.model.h.1.ln_1.bias', 'transformer.model.h.1.ln_1.weight', 'transformer.model.h.1.ln_2.bias', 'transformer.model.h.1.ln_2.weight', 'transformer.model.h.1.mlp.c_fc.bias', 'transformer.model.h.1.mlp.c_fc.weight', 'transformer.model.h.1.mlp.c_proj.bias', 'transformer.model.h.1.mlp.c_proj.weight', 'transformer.model.h.10.attn.attention.k_proj.weight', 'transformer.model.h.10.attn.attention.out_proj.bias', 'transformer.model.h.10.attn.attention.out_proj.weight', 'transformer.model.h.10.attn.attention.q_proj.weight', 'transformer.model.h.10.attn.attention.v_proj.weight', 'transformer.model.h.10.ln_1.bias', 'transformer.model.h.10.ln_1.weight', 'transformer.model.h.10.ln_2.bias', 'transformer.model.h.10.ln_2.weight', 'transformer.model.h.10.mlp.c_fc.bias', 'transformer.model.h.10.mlp.c_fc.weight', 'transformer.model.h.10.mlp.c_proj.bias', 'transformer.model.h.10.mlp.c_proj.weight', 'transformer.model.h.11.attn.attention.k_proj.weight', 'transformer.model.h.11.attn.attention.out_proj.bias', 'transformer.model.h.11.attn.attention.out_proj.weight', 'transformer.model.h.11.attn.attention.q_proj.weight', 'transformer.model.h.11.attn.attention.v_proj.weight', 'transformer.model.h.11.ln_1.bias', 'transformer.model.h.11.ln_1.weight', 'transformer.model.h.11.ln_2.bias', 'transformer.model.h.11.ln_2.weight', 'transformer.model.h.11.mlp.c_fc.bias', 'transformer.model.h.11.mlp.c_fc.weight', 'transformer.model.h.11.mlp.c_proj.bias', 'transformer.model.h.11.mlp.c_proj.weight', 'transformer.model.h.12.attn.attention.k_proj.weight', 'transformer.model.h.12.attn.attention.out_proj.bias', 'transformer.model.h.12.attn.attention.out_proj.weight', 'transformer.model.h.12.attn.attention.q_proj.weight', 'transformer.model.h.12.attn.attention.v_proj.weight', 'transformer.model.h.12.ln_1.bias', 'transformer.model.h.12.ln_1.weight', 'transformer.model.h.12.ln_2.bias', 'transformer.model.h.12.ln_2.weight', 'transformer.model.h.12.mlp.c_fc.bias', 'transformer.model.h.12.mlp.c_fc.weight', 'transformer.model.h.12.mlp.c_proj.bias', 'transformer.model.h.12.mlp.c_proj.weight', 'transformer.model.h.13.attn.attention.k_proj.weight', 'transformer.model.h.13.attn.attention.out_proj.bias', 'transformer.model.h.13.attn.attention.out_proj.weight', 'transformer.model.h.13.attn.attention.q_proj.weight', 'transformer.model.h.13.attn.attention.v_proj.weight', 'transformer.model.h.13.ln_1.bias', 'transformer.model.h.13.ln_1.weight', 'transformer.model.h.13.ln_2.bias', 'transformer.model.h.13.ln_2.weight', 'transformer.model.h.13.mlp.c_fc.bias', 'transformer.model.h.13.mlp.c_fc.weight', 'transformer.model.h.13.mlp.c_proj.bias', 'transformer.model.h.13.mlp.c_proj.weight', 'transformer.model.h.14.attn.attention.k_proj.weight', 'transformer.model.h.14.attn.attention.out_proj.bias', 'transformer.model.h.14.attn.attention.out_proj.weight', 'transformer.model.h.14.attn.attention.q_proj.weight', 'transformer.model.h.14.attn.attention.v_proj.weight', 'transformer.model.h.14.ln_1.bias', 'transformer.model.h.14.ln_1.weight', 'transformer.model.h.14.ln_2.bias', 'transformer.model.h.14.ln_2.weight', 'transformer.model.h.14.mlp.c_fc.bias', 'transformer.model.h.14.mlp.c_fc.weight', 'transformer.model.h.14.mlp.c_proj.bias', 'transformer.model.h.14.mlp.c_proj.weight', 'transformer.model.h.15.attn.attention.k_proj.weight', 'transformer.model.h.15.attn.attention.out_proj.bias', 'transformer.model.h.15.attn.attention.out_proj.weight', 'transformer.model.h.15.attn.attention.q_proj.weight', 'transformer.model.h.15.attn.attention.v_proj.weight', 'transformer.model.h.15.ln_1.bias', 'transformer.model.h.15.ln_1.weight', 'transformer.model.h.15.ln_2.bias', 'transformer.model.h.15.ln_2.weight', 'transformer.model.h.15.mlp.c_fc.bias', 'transformer.model.h.15.mlp.c_fc.weight', 'transformer.model.h.15.mlp.c_proj.bias', 'transformer.model.h.15.mlp.c_proj.weight', 'transformer.model.h.16.attn.attention.k_proj.weight', 'transformer.model.h.16.attn.attention.out_proj.bias', 'transformer.model.h.16.attn.attention.out_proj.weight', 'transformer.model.h.16.attn.attention.q_proj.weight', 'transformer.model.h.16.attn.attention.v_proj.weight', 'transformer.model.h.16.ln_1.bias', 'transformer.model.h.16.ln_1.weight', 'transformer.model.h.16.ln_2.bias', 'transformer.model.h.16.ln_2.weight', 'transformer.model.h.16.mlp.c_fc.bias', 'transformer.model.h.16.mlp.c_fc.weight', 'transformer.model.h.16.mlp.c_proj.bias', 'transformer.model.h.16.mlp.c_proj.weight', 'transformer.model.h.17.attn.attention.k_proj.weight', 'transformer.model.h.17.attn.attention.out_proj.bias', 'transformer.model.h.17.attn.attention.out_proj.weight', 'transformer.model.h.17.attn.attention.q_proj.weight', 'transformer.model.h.17.attn.attention.v_proj.weight', 'transformer.model.h.17.ln_1.bias', 'transformer.model.h.17.ln_1.weight', 'transformer.model.h.17.ln_2.bias', 'transformer.model.h.17.ln_2.weight', 'transformer.model.h.17.mlp.c_fc.bias', 'transformer.model.h.17.mlp.c_fc.weight', 'transformer.model.h.17.mlp.c_proj.bias', 'transformer.model.h.17.mlp.c_proj.weight', 'transformer.model.h.18.attn.attention.k_proj.weight', 'transformer.model.h.18.attn.attention.out_proj.bias', 'transformer.model.h.18.attn.attention.out_proj.weight', 'transformer.model.h.18.attn.attention.q_proj.weight', 'transformer.model.h.18.attn.attention.v_proj.weight', 'transformer.model.h.18.ln_1.bias', 'transformer.model.h.18.ln_1.weight', 'transformer.model.h.18.ln_2.bias', 'transformer.model.h.18.ln_2.weight', 'transformer.model.h.18.mlp.c_fc.bias', 'transformer.model.h.18.mlp.c_fc.weight', 'transformer.model.h.18.mlp.c_proj.bias', 'transformer.model.h.18.mlp.c_proj.weight', 'transformer.model.h.19.attn.attention.k_proj.weight', 'transformer.model.h.19.attn.attention.out_proj.bias', 'transformer.model.h.19.attn.attention.out_proj.weight', 'transformer.model.h.19.attn.attention.q_proj.weight', 'transformer.model.h.19.attn.attention.v_proj.weight', 'transformer.model.h.19.ln_1.bias', 'transformer.model.h.19.ln_1.weight', 'transformer.model.h.19.ln_2.bias', 'transformer.model.h.19.ln_2.weight', 'transformer.model.h.19.mlp.c_fc.bias', 'transformer.model.h.19.mlp.c_fc.weight', 'transformer.model.h.19.mlp.c_proj.bias', 'transformer.model.h.19.mlp.c_proj.weight', 'transformer.model.h.2.attn.attention.k_proj.weight', 'transformer.model.h.2.attn.attention.out_proj.bias', 'transformer.model.h.2.attn.attention.out_proj.weight', 'transformer.model.h.2.attn.attention.q_proj.weight', 'transformer.model.h.2.attn.attention.v_proj.weight', 'transformer.model.h.2.ln_1.bias', 'transformer.model.h.2.ln_1.weight', 'transformer.model.h.2.ln_2.bias', 'transformer.model.h.2.ln_2.weight', 'transformer.model.h.2.mlp.c_fc.bias', 'transformer.model.h.2.mlp.c_fc.weight', 'transformer.model.h.2.mlp.c_proj.bias', 'transformer.model.h.2.mlp.c_proj.weight', 'transformer.model.h.20.attn.attention.k_proj.weight', 'transformer.model.h.20.attn.attention.out_proj.bias', 'transformer.model.h.20.attn.attention.out_proj.weight', 'transformer.model.h.20.attn.attention.q_proj.weight', 'transformer.model.h.20.attn.attention.v_proj.weight', 'transformer.model.h.20.ln_1.bias', 'transformer.model.h.20.ln_1.weight', 'transformer.model.h.20.ln_2.bias', 'transformer.model.h.20.ln_2.weight', 'transformer.model.h.20.mlp.c_fc.bias', 'transformer.model.h.20.mlp.c_fc.weight', 'transformer.model.h.20.mlp.c_proj.bias', 'transformer.model.h.20.mlp.c_proj.weight', 'transformer.model.h.21.attn.attention.k_proj.weight', 'transformer.model.h.21.attn.attention.out_proj.bias', 'transformer.model.h.21.attn.attention.out_proj.weight', 'transformer.model.h.21.attn.attention.q_proj.weight', 'transformer.model.h.21.attn.attention.v_proj.weight', 'transformer.model.h.21.ln_1.bias', 'transformer.model.h.21.ln_1.weight', 'transformer.model.h.21.ln_2.bias', 'transformer.model.h.21.ln_2.weight', 'transformer.model.h.21.mlp.c_fc.bias', 'transformer.model.h.21.mlp.c_fc.weight', 'transformer.model.h.21.mlp.c_proj.bias', 'transformer.model.h.21.mlp.c_proj.weight', 'transformer.model.h.22.attn.attention.k_proj.weight', 'transformer.model.h.22.attn.attention.out_proj.bias', 'transformer.model.h.22.attn.attention.out_proj.weight', 'transformer.model.h.22.attn.attention.q_proj.weight', 'transformer.model.h.22.attn.attention.v_proj.weight', 'transformer.model.h.22.ln_1.bias', 'transformer.model.h.22.ln_1.weight', 'transformer.model.h.22.ln_2.bias', 'transformer.model.h.22.ln_2.weight', 'transformer.model.h.22.mlp.c_fc.bias', 'transformer.model.h.22.mlp.c_fc.weight', 'transformer.model.h.22.mlp.c_proj.bias', 'transformer.model.h.22.mlp.c_proj.weight', 'transformer.model.h.23.attn.attention.k_proj.weight', 'transformer.model.h.23.attn.attention.out_proj.bias', 'transformer.model.h.23.attn.attention.out_proj.weight', 'transformer.model.h.23.attn.attention.q_proj.weight', 'transformer.model.h.23.attn.attention.v_proj.weight', 'transformer.model.h.23.ln_1.bias', 'transformer.model.h.23.ln_1.weight', 'transformer.model.h.23.ln_2.bias', 'transformer.model.h.23.ln_2.weight', 'transformer.model.h.23.mlp.c_fc.bias', 'transformer.model.h.23.mlp.c_fc.weight', 'transformer.model.h.23.mlp.c_proj.bias', 'transformer.model.h.23.mlp.c_proj.weight', 'transformer.model.h.3.attn.attention.k_proj.weight', 'transformer.model.h.3.attn.attention.out_proj.bias', 'transformer.model.h.3.attn.attention.out_proj.weight', 'transformer.model.h.3.attn.attention.q_proj.weight', 'transformer.model.h.3.attn.attention.v_proj.weight', 'transformer.model.h.3.ln_1.bias', 'transformer.model.h.3.ln_1.weight', 'transformer.model.h.3.ln_2.bias', 'transformer.model.h.3.ln_2.weight', 'transformer.model.h.3.mlp.c_fc.bias', 'transformer.model.h.3.mlp.c_fc.weight', 'transformer.model.h.3.mlp.c_proj.bias', 'transformer.model.h.3.mlp.c_proj.weight', 'transformer.model.h.4.attn.attention.k_proj.weight', 'transformer.model.h.4.attn.attention.out_proj.bias', 'transformer.model.h.4.attn.attention.out_proj.weight', 'transformer.model.h.4.attn.attention.q_proj.weight', 'transformer.model.h.4.attn.attention.v_proj.weight', 'transformer.model.h.4.ln_1.bias', 'transformer.model.h.4.ln_1.weight', 'transformer.model.h.4.ln_2.bias', 'transformer.model.h.4.ln_2.weight', 'transformer.model.h.4.mlp.c_fc.bias', 'transformer.model.h.4.mlp.c_fc.weight', 'transformer.model.h.4.mlp.c_proj.bias', 'transformer.model.h.4.mlp.c_proj.weight', 'transformer.model.h.5.attn.attention.k_proj.weight', 'transformer.model.h.5.attn.attention.out_proj.bias', 'transformer.model.h.5.attn.attention.out_proj.weight', 'transformer.model.h.5.attn.attention.q_proj.weight', 'transformer.model.h.5.attn.attention.v_proj.weight', 'transformer.model.h.5.ln_1.bias', 'transformer.model.h.5.ln_1.weight', 'transformer.model.h.5.ln_2.bias', 'transformer.model.h.5.ln_2.weight', 'transformer.model.h.5.mlp.c_fc.bias', 'transformer.model.h.5.mlp.c_fc.weight', 'transformer.model.h.5.mlp.c_proj.bias', 'transformer.model.h.5.mlp.c_proj.weight', 'transformer.model.h.6.attn.attention.k_proj.weight', 'transformer.model.h.6.attn.attention.out_proj.bias', 'transformer.model.h.6.attn.attention.out_proj.weight', 'transformer.model.h.6.attn.attention.q_proj.weight', 'transformer.model.h.6.attn.attention.v_proj.weight', 'transformer.model.h.6.ln_1.bias', 'transformer.model.h.6.ln_1.weight', 'transformer.model.h.6.ln_2.bias', 'transformer.model.h.6.ln_2.weight', 'transformer.model.h.6.mlp.c_fc.bias', 'transformer.model.h.6.mlp.c_fc.weight', 'transformer.model.h.6.mlp.c_proj.bias', 'transformer.model.h.6.mlp.c_proj.weight', 'transformer.model.h.7.attn.attention.k_proj.weight', 'transformer.model.h.7.attn.attention.out_proj.bias', 'transformer.model.h.7.attn.attention.out_proj.weight', 'transformer.model.h.7.attn.attention.q_proj.weight', 'transformer.model.h.7.attn.attention.v_proj.weight', 'transformer.model.h.7.ln_1.bias', 'transformer.model.h.7.ln_1.weight', 'transformer.model.h.7.ln_2.bias', 'transformer.model.h.7.ln_2.weight', 'transformer.model.h.7.mlp.c_fc.bias', 'transformer.model.h.7.mlp.c_fc.weight', 'transformer.model.h.7.mlp.c_proj.bias', 'transformer.model.h.7.mlp.c_proj.weight', 'transformer.model.h.8.attn.attention.k_proj.weight', 'transformer.model.h.8.attn.attention.out_proj.bias', 'transformer.model.h.8.attn.attention.out_proj.weight', 'transformer.model.h.8.attn.attention.q_proj.weight', 'transformer.model.h.8.attn.attention.v_proj.weight', 'transformer.model.h.8.ln_1.bias', 'transformer.model.h.8.ln_1.weight', 'transformer.model.h.8.ln_2.bias', 'transformer.model.h.8.ln_2.weight', 'transformer.model.h.8.mlp.c_fc.bias', 'transformer.model.h.8.mlp.c_fc.weight', 'transformer.model.h.8.mlp.c_proj.bias', 'transformer.model.h.8.mlp.c_proj.weight', 'transformer.model.h.9.attn.attention.k_proj.weight', 'transformer.model.h.9.attn.attention.out_proj.bias', 'transformer.model.h.9.attn.attention.out_proj.weight', 'transformer.model.h.9.attn.attention.q_proj.weight', 'transformer.model.h.9.attn.attention.v_proj.weight', 'transformer.model.h.9.ln_1.bias', 'transformer.model.h.9.ln_1.weight', 'transformer.model.h.9.ln_2.bias', 'transformer.model.h.9.ln_2.weight', 'transformer.model.h.9.mlp.c_fc.bias', 'transformer.model.h.9.mlp.c_fc.weight', 'transformer.model.h.9.mlp.c_proj.bias', 'transformer.model.h.9.mlp.c_proj.weight', 'transformer.model.ln_f.bias', 'transformer.model.ln_f.weight', 'transformer.model.wpe.weight', 'transformer.model.wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"ncfrey/ChemGPT-19M\")\n",
    "model = CPPGenerator.from_pretrained(\"ncfrey/ChemGPT-19M\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_df['is_cpp'].values, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_df['is_cpp'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1694699436017912\n",
      "Epoch 2, Loss: 0.39172834709082566\n",
      "Epoch 3, Loss: 0.1755400059157855\n",
      "Epoch 4, Loss: 0.09764893138653612\n",
      "Epoch 5, Loss: 0.06253999374704818\n",
      "Epoch 6, Loss: 0.04374667650012121\n",
      "Epoch 7, Loss: 0.03245650874833538\n",
      "Epoch 8, Loss: 0.025107077692877757\n",
      "Epoch 9, Loss: 0.02003458711280398\n",
      "Epoch 10, Loss: 0.016374668374351444\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Generate logits\n",
    "        logits = model(batch)\n",
    "        \n",
    "        # Shift targets (next token prediction)\n",
    "        targets = batch[:, 1:].contiguous().view(-1)\n",
    "        logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AdamW\n",
    "from tokenizers import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPPDataset(Dataset):\n",
    "    def __init__(self, smiles, labels, tokenizer, max_length=128):\n",
    "        self.sequences = [\n",
    "            tokenizer.encode(f\"[{'CPP' if label else 'NON'}]{s}\").ids[:max_length]\n",
    "            for s, label in zip(smiles, labels)\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPPGenerator(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.condition_embed = nn.Embedding(2, config.n_embd)\n",
    "        \n",
    "    def forward(self, input_ids=None, labels=None, condition=None):\n",
    "        if condition is not None:\n",
    "            cond_emb = self.condition_embed(condition)\n",
    "            inputs_embeds = self.transformer.wte(input_ids) + cond_emb.unsqueeze(1)\n",
    "            return super().forward(inputs_embeds=inputs_embeds, labels=labels)\n",
    "        return super().forward(input_ids=input_ids, labels=labels)\n",
    "\n",
    "def initialize_generator(tokenizer):\n",
    "    config = GPT2Config(\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        n_positions=128,\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        pad_token_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "        bos_token_id=tokenizer.token_to_id(\"[CPP]\"),\n",
    "    )\n",
    "    return CPPGenerator(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPPClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return torch.sigmoid(self.classifier(h_n[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 4. Training Utilities\n",
    "# --------------------------------------------------\n",
    "def train_generator(model, train_loader, val_loader, epochs=10):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs = batch.to(device)\n",
    "            condition = (inputs[:, 0] == tokenizer.token_to_id(\"[CPP]\")).long()\n",
    "            \n",
    "            # Remove condition token for labels\n",
    "            labels = torch.where(inputs == tokenizer.token_to_id(\"[CPP]\"), -100, inputs)\n",
    "            labels = torch.where(inputs == tokenizer.token_to_id(\"[NON]\"), -100, labels)\n",
    "            \n",
    "            outputs = model(inputs[:, 1:], labels=labels[:, 1:].to(device), condition=condition)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch.to(device)\n",
    "                condition = (inputs[:, 0] == tokenizer.token_to_id(\"[CPP]\")).long()\n",
    "                labels = torch.where(inputs == tokenizer.token_to_id(\"[CPP]\"), -100, inputs)\n",
    "                labels = torch.where(inputs == tokenizer.token_to_id(\"[NON]\"), -100, labels)\n",
    "                \n",
    "                outputs = model(inputs[:, 1:], labels=labels[:, 1:].to(device), condition=condition)\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "def train_classifier(model, train_loader, val_loader, epochs=10):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].float().to(device)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[0].to(device)\n",
    "                labels = batch[1].cpu().numpy()\n",
    "                \n",
    "                outputs = model(inputs).squeeze().cpu().numpy()\n",
    "                preds = (outputs > 0.5).astype(int)\n",
    "                \n",
    "                val_loss += criterion(torch.tensor(outputs), torch.tensor(labels)).item()\n",
    "                val_acc += accuracy_score(labels, preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize models with strict validation\n",
    "def initialize_generator(tokenizer):\n",
    "    # Verify special tokens first\n",
    "    assert tokenizer.token_to_id(\"[PAD]\") is not None, \"Missing [PAD] token\"\n",
    "    assert tokenizer.token_to_id(\"[CPP]\") is not None, \"Missing [CPP] token\"\n",
    "    assert tokenizer.token_to_id(\"[NON]\") is not None, \"Missing [NON] token\"\n",
    "    \n",
    "    config = GPT2Config(\n",
    "        vocab_size=tokenizer.get_vocab_size(),\n",
    "        n_positions=128,\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        pad_token_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "        bos_token_id=tokenizer.token_to_id(\"[CPP]\"),\n",
    "        eos_token_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    model = CPPGenerator(config)\n",
    "    \n",
    "    # Initialize embeddings properly\n",
    "    model.transformer.wte.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    model.condition_embed.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Enhanced batch validation with memory safety\n",
    "def validate_batch(batch):\n",
    "    # Move to CPU for safe validation\n",
    "    cpu_batch = batch.cpu()\n",
    "    valid_ids = set(tokenizer.get_vocab().values())\n",
    "    \n",
    "    # Check token range\n",
    "    min_id, max_id = min(valid_ids), max(valid_ids)\n",
    "    invalid = (cpu_batch < min_id) | (cpu_batch > max_id)\n",
    "    \n",
    "    if invalid.any():\n",
    "        bad_tokens = cpu_batch[invalid].unique().tolist()\n",
    "        print(f\"Invalid tokens found: {bad_tokens}\")\n",
    "        print(f\"Valid token range: {min_id}-{max_id}\")\n",
    "        raise ValueError(\"Batch contains invalid token IDs\")\n",
    "        \n",
    "    # Check special tokens\n",
    "    pad_present = (cpu_batch == tokenizer.token_to_id(\"[PAD]\")).any()\n",
    "    cpp_present = (cpu_batch == tokenizer.token_to_id(\"[CPP]\")).any()\n",
    "    \n",
    "    if not pad_present:\n",
    "        print(\"Warning: No padding tokens in batch\")\n",
    "    if not cpp_present:\n",
    "        print(\"Warning: No CPP tokens in batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Add CUDA memory management\n",
    "def clear_cuda_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1e6\n",
    "    reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"CUDA memory: {allocated:.1f}MB allocated, {reserved:.1f}MB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Modified training loop\n",
    "def safe_train_generator(model, train_loader, val_loader, epochs=10):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            clear_cuda_cache()\n",
    "            \n",
    "            try:\n",
    "                # Validate before moving to GPU\n",
    "                validate_batch(batch)\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                # Create labels\n",
    "                labels = batch.clone()\n",
    "                labels[labels == tokenizer.token_to_id(\"[CPP]\")] = -100\n",
    "                labels[labels == tokenizer.token_to_id(\"[NON]\")] = -100\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(\n",
    "                        input_ids=batch[:, 1:],\n",
    "                        labels=labels[:, 1:],\n",
    "                        condition=(batch[:, 0] == tokenizer.token_to_id(\"[CPP]\")).long()\n",
    "                    )\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                # Backward pass with gradient clipping\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Every 10 batches, validate memory\n",
    "                if batch_idx % 10 == 0:\n",
    "                    clear_cuda_cache()\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                if 'CUDA out of memory' in str(e):\n",
    "                    print(\"OOM detected, reducing batch size\")\n",
    "                    del batch, outputs, loss\n",
    "                    clear_cuda_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dreamtim/micromamba/envs/cpp-embeddings/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_734498/1336219391.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory: 19.2MB allocated, 21.0MB reserved\n",
      "CUDA memory: 73.9MB allocated, 165.7MB reserved\n",
      "CUDA memory: 73.9MB allocated, 165.7MB reserved\n",
      "CUDA memory: 112.8MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 209.7MB reserved\n",
      "CUDA memory: 112.7MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "Epoch 1 | Loss: 0.0196\n",
      "CUDA memory: 108.9MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "Epoch 2 | Loss: 0.0007\n",
      "CUDA memory: 110.4MB allocated, 213.9MB reserved\n",
      "CUDA memory: 112.0MB allocated, 188.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 188.7MB reserved\n",
      "CUDA memory: 112.8MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "Epoch 3 | Loss: 0.0004\n",
      "CUDA memory: 108.9MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "Epoch 4 | Loss: 0.0002\n",
      "CUDA memory: 110.4MB allocated, 213.9MB reserved\n",
      "CUDA memory: 112.0MB allocated, 188.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 188.7MB reserved\n",
      "CUDA memory: 112.8MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "Epoch 5 | Loss: 0.0002\n",
      "CUDA memory: 108.9MB allocated, 224.4MB reserved\n",
      "CUDA memory: 112.0MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "Epoch 6 | Loss: 0.0001\n",
      "CUDA memory: 110.4MB allocated, 213.9MB reserved\n",
      "CUDA memory: 112.0MB allocated, 188.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 188.7MB reserved\n",
      "CUDA memory: 112.8MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "Epoch 7 | Loss: 0.0001\n",
      "CUDA memory: 108.9MB allocated, 224.4MB reserved\n",
      "CUDA memory: 112.0MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "Epoch 8 | Loss: 0.0001\n",
      "CUDA memory: 110.4MB allocated, 213.9MB reserved\n",
      "CUDA memory: 112.0MB allocated, 188.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 188.7MB reserved\n",
      "CUDA memory: 112.8MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 211.8MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 216.0MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "Epoch 9 | Loss: 0.0001\n",
      "CUDA memory: 108.9MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.0MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 209.7MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.7MB allocated, 222.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "CUDA memory: 112.6MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 216.0MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 112.2MB allocated, 201.3MB reserved\n",
      "CUDA memory: 111.9MB allocated, 211.8MB reserved\n",
      "Epoch 10 | Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train generator\n",
    "train_dataset = CPPDataset(train_df['smiles_sequence'], train_df['is_cpp'], tokenizer)\n",
    "val_dataset = CPPDataset(val_df['smiles_sequence'], val_df['is_cpp'], tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "model = initialize_generator(tokenizer)\n",
    "safe_train_generator(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cpp(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    num_samples=10,\n",
    "    max_length=100,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"Generate CPP SMILES with robust sampling controls\"\"\"\n",
    "    model.eval()\n",
    "    valid_smiles = []\n",
    "    start_token = tokenizer.token_to_id(\"[CPP]\")\n",
    "    pad_token = tokenizer.token_to_id(\"[PAD]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            input_ids = torch.tensor([[start_token]], dtype=torch.long, device=device)\n",
    "            condition = torch.tensor([1], device=device)\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                outputs = model(input_ids, condition=condition)\n",
    "                logits = outputs.logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Filter invalid tokens first\n",
    "                valid_mask = torch.ones_like(logits, dtype=torch.bool)\n",
    "                valid_mask[:, [pad_token]] = False  # Always exclude pad token\n",
    "                logits[~valid_mask] = -float('inf')\n",
    "                \n",
    "                # Dynamic top_k adjustment\n",
    "                num_valid = valid_mask.sum().item()\n",
    "                current_top_k = min(top_k, num_valid) if num_valid > 0 else 1\n",
    "                \n",
    "                if current_top_k > 0:\n",
    "                    top_values = torch.topk(logits, current_top_k)[0]\n",
    "                    logits[logits < top_values[:, [-1]]] = -float('inf')\n",
    "                \n",
    "                if top_p > 0.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(\n",
    "                        torch.softmax(sorted_logits, dim=-1), \n",
    "                        dim=-1\n",
    "                    )\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = False\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                        -1, sorted_indices, sorted_indices_to_remove\n",
    "                    )\n",
    "                    logits[indices_to_remove] = -float('inf')\n",
    "\n",
    "                # Check for valid tokens\n",
    "                if (logits == -float('inf')).all():\n",
    "                    break\n",
    "                \n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                if next_token.item() == pad_token:\n",
    "                    break\n",
    "                    \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "            # Decode and validate\n",
    "            generated = tokenizer.decode(input_ids[0].cpu().tolist())\n",
    "            generated = generated.replace(\"[CPP]\", \"\").replace(\"[PAD]\", \"\").strip()\n",
    "            \n",
    "            try:\n",
    "                if Chem.MolFromSmiles(generated) is not None:\n",
    "                    valid_smiles.append(generated)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return valid_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, save_dir=\"./cpp_generator\"):\n",
    "    \"\"\"Save model and tokenizer\"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"config\": model.config.to_dict()\n",
    "    }, f\"{save_dir}/model.pth\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save(f\"{save_dir}/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(save_dir=\"./cpp_generator\", device=\"cpu\"):\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "    from tokenizers import Tokenizer\n",
    "    from transformers import GPT2Config\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = Tokenizer.from_file(f\"{save_dir}/tokenizer.json\")\n",
    "    \n",
    "    # Load model config\n",
    "    checkpoint = torch.load(f\"{save_dir}/model.pth\", map_location=device)\n",
    "    config = GPT2Config.from_dict(checkpoint[\"config\"])\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CPPGenerator(config).to(device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated CPPs:\n",
      "1. \n",
      "2. \n",
      "3. \n",
      "4. \n",
      "5. \n",
      "6. \n",
      "7. \n",
      "8. \n",
      "9. \n",
      "10. \n"
     ]
    }
   ],
   "source": [
    "# Generate with safe parameters\n",
    "generated_smiles = generate_cpp(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=10,\n",
    "    temperature=0.9,\n",
    "    top_k=40,  # Will auto-adjust if needed\n",
    "    top_p=0.9,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Generated CPPs:\")\n",
    "for i, smi in enumerate(generated_smiles, 1):\n",
    "    print(f\"{i}. {smi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tokens: \n",
      "Step 1 token: 1 ([NON])\n",
      "Step 2 token: 1 ([NON])\n",
      "Step 3 token: 1 ([NON])\n",
      "Step 4 token: 1 ([NON])\n",
      "Step 5 token: 1 ([NON])\n",
      "Raw generated: \n"
     ]
    }
   ],
   "source": [
    "def debug_generation(model, tokenizer, device):\n",
    "    model.eval()\n",
    "    start_token = tokenizer.token_to_id(\"[CPP]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor([[start_token]], device=device)\n",
    "        print(\"Initial tokens:\", tokenizer.decode(input_ids[0].cpu().tolist()))\n",
    "        \n",
    "        for step in range(5):  # First 5 generation steps\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            print(f\"Step {step+1} token: {next_token.item()} ({tokenizer.id_to_token(next_token.item())})\")\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    raw_output = tokenizer.decode(input_ids[0].cpu().tolist())\n",
    "    print(\"Raw generated:\", raw_output)\n",
    "    return raw_output\n",
    "\n",
    "# Usage\n",
    "debug_output = debug_generation(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_generation(model, tokenizer, device):\n",
    "    model.eval()\n",
    "    start_token = tokenizer.token_to_id(\"[CPP]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor([[start_token]], device=device)\n",
    "        condition = torch.tensor([1], device=device)  # CPP condition\n",
    "        \n",
    "        print(\"Initial tokens:\", tokenizer.decode(input_ids[0].cpu().tolist()))\n",
    "        \n",
    "        for step in range(5):\n",
    "            outputs = model(input_ids, condition=condition)  # Pass condition\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            print(f\"Step {step+1} token: {next_token.item()} ({tokenizer.id_to_token(next_token.item())})\")\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    raw_output = tokenizer.decode(input_ids[0].cpu().tolist())\n",
    "    print(\"Raw generated:\", raw_output)\n",
    "    return raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training sequences:\n",
      "Sample 1: \n",
      "Token IDs: [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------\n",
      "Sample 2: \n",
      "Token IDs: [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------\n",
      "Sample 3: \n",
      "Token IDs: [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Should show \"[CPP]SMILES...\" for positive, \"[NON]SMILES...\" for negative\n",
    "print(\"Sample training sequences:\")\n",
    "for i in range(3):\n",
    "    # Get complete sequence for sample i\n",
    "    token_sequence = train_dataset[i].tolist()\n",
    "    \n",
    "    # Decode properly with batch dimension\n",
    "    decoded_seq = tokenizer.decode(ids=token_sequence)\n",
    "    \n",
    "    print(f\"Sample {i+1}: {decoded_seq}\")\n",
    "    print(f\"Token IDs: {token_sequence}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpp-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
